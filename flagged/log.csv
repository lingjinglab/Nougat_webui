pdf文件,要转换的页码,Markdown 预览,flag,username,timestamp
E:\workspace\python\Ocr\Nougat_webui\flagged\pdf文件\2303.03926kxophz9y.pdf,11,"<p>Speaker SimilarityWe first evaluate whether the speaker’s voice is preserved in the generated target speech using speaker similarity (ASV-Score), whose results are listed in Table 4. Because the EMIME test set has paired speech utterances with Chinese and English, we are able to calculate the ASV score among the generated speech (hyp), the source speech (src), as well as the target speech (tgt), resulting in 3 settings (tgt vs. src, hyp vs. src, and hyp vs. tgt). From Table 4 we can find that: (1) For Chinese(\rightarrow)English, the ASV score of VALL-E X Trans significantly outperforms that of the conventional speaker embedding based S2ST system (Baseline), demonstrating the superiority of our model in terms of maintaining the source speaker’s voice. (2) The ASV score has similar values when the generated speech (hyp) is compared with the source speech (src) and the target speech (tgt), and it is far away from the upper bound (tgt vs. src) for the English(\rightarrow)Chinese direction, which suggests that the cross-lingual voice transferability still has the improvement space. (3) When directly generating speech from the ground-truth (oracle) text which degrades into cross-lingual TTS, the ASV score does not increase notably, indicating that voice transferability is less affected by the quality of translation.</p>
<p>Translation QualityTable 4 also shows the translation performance of VALL-E X Trans. Note that ASR-BLEU with oracle target text as the input of VALL-E X can be seen as the upper bound when translations are exactly correct. With oracle target text as input, VALL-E X Trans can achieve the performance of about 84(\sim)87 BLEU scores, which also reflects the high performance of our neural codec language model. For Chinese(\rightarrow)English, VALL-E X Trans achieves higher BLEU over the baseline (30.66 vs. 27.49), demonstrating the end-to-end speech-to-phoneme translation is more effective against the conventional cascaded speech-to-text translation when applying to S2ST task.</p>
<p>Speech NaturalnessWe also evaluate the Naturalness with the open-source NISQA [Mittag and Moller, 2021] for S2ST outputs. As shown in the last column of Table 4, compared to the baseline, VALL-E X Trans achieves a better naturalness score (3.54 vs. 3.44), which shows that VALL-E X can generate more natural target language speech than the baseline.</p>
<p>Human EvaluationWe randomly sample 56 translation pairs13 to perform a human evaluation using SMOS and MOS metrics for both Chinese(\rightarrow)English and English(\rightarrow)Chinese directions. Table 5 lists the results of VALL-E X Trans as well as the Chinese(\rightarrow)English baseline. We use MOS (from 1 to 5 scores) instead of CMOS because the translated content may be different among models, which is not suitable for CMOS evaluation. For speaker similarity evaluation, VALL-E X Trans outperforms the baseline with 1.06 SMOS scores (4.12 vs. 3.06), demonstrating its superior ability to model speaker property of the proposed VALL-E X. Note that this value still can be improved since it is still far from the SMOS between the source speech prompt and ground truth (4.91). For speech quality, our VALL-E X slightly outperforms the baseline in Chinese(\rightarrow)English S2ST in terms of MOS score (3.87 vs. 3.81).</p>
<p>\begin{table}<br>
\begin{tabular}{c|c c c c} \hline \hline  &amp; \multicolumn{3}{c}{ASV-Score} &amp; \multirow{2}{<em>}{ASR-BLEU} &amp; \multirow{2}{</em>}{Naturalness} \  &amp; tgt vs. src &amp; hyp vs. src &amp; hyp vs. tgt &amp; \ \hline <em>Chinese(\rightarrow)English S2ST</em> &amp; &amp; &amp; &amp; \ \hline Baseline (S2ST) &amp; &amp; 0.28(\pm)0.10 &amp; 0.27(\pm)0.12 &amp; 27.49 &amp; 3.44 \ - w/ oracle target text &amp; &amp; 0.28(\pm)0.10 &amp; 0.29(\pm)0.11 &amp; 80.30 &amp; 3.43 \ VALL-E X Trans &amp; &amp; 0.37(\pm)0.10 &amp; 0.37(\pm)0.11 &amp; 30.66 &amp; 3.54 \ - w/ oracle target text &amp; &amp; 0.39(\pm)0.10 &amp; 0.38(\pm)0.10 &amp; 86.78 &amp; 3.54 \ \hline <em>English(\rightarrow)Chinese S2ST</em> &amp; &amp; &amp; &amp; \ \hline VALL-E X Trans &amp; 0.58(\pm)0.09 &amp; 0.48(\pm)0.11 &amp; 0.53(\pm)0.11 &amp; 34.45 &amp; 3.41 \ - w/ oracle target text &amp; &amp; 0.47(\pm)0.12 &amp; 0.55(\pm)0.11 &amp; 84.00 &amp; 3.42 \ \hline \hline \end{tabular}<br>
\end{table}<br>
Table 4: S2ST performance on EMIME dataset for Chinese(\leftrightarrow)English directions. Baseline is a cascaded S2ST system based on speaker embedding. Automatic evaluation matrices include ASV-Score, ASR-BLEU, and Naturalness.</p>
",,,2023-09-05 11:00:29.783718
E:\workspace\python\Ocr\Nougat_webui\flagged\pdf文件\2303.039264hzqni5n.pdf,6,"'<!DOCTYPE html>
<html lang=""en"" data-lt-installed=""true""><head>
  <meta charset=""UTF-8"">
  <title>Title</title>
  <script>
    const text = Multi-lingual Non-Autoregressive Code LMInstead of the autoregressive generation pattern, multi-lingual non-autoregressive codec LM \(\phi_{\mathrm{MNAR}}\) is a non-autoregressive Transformer language model aiming at iteratively generating the rest layers of acoustic tokens from the first layer. It is prompted by the phoneme sequence of the current sentence (\(\mathcal{S}\)) and the acoustic token sequence of another sentence with the same speaker (\(\tilde{\mathcal{A}}\)). Here \(\tilde{\mathcal{A}}\) is taken from the previous sentence in the dataset where the adjusted sentences are usually segmented from the same paragraph. It is expected to have the same characteristics of voice (speaker, speed, background, etc) as the current sentence and is used as an additional reference for cloning the target voice. Like VALL-E, for generating acoustic tokens of each layer \(l\in[2,8]\), the embeddings of \(l-1\) layers' acoustic tokens (\(\mathcal{A}_{\cdot,1:l-1}\)) are summed up layerwise as input. The learning objective for the \(l\)-layer acoustic tokens \(\mathcal{A}_{\cdot,l}\) can be calculated as

\[\mathcal{L}_{\mathrm{MNAR}}=\sum_{l=2}^{8}\log p_{\mathrm{NAR}}\left(\mathcal{ A}_{\cdot,l}\mid\left\langle\mathcal{S},\tilde{\mathcal{A}}_{\cdot,1:8}, \mathcal{A}_{\cdot,1:l-1}\right\rangle;\phi_{\mathrm{MNAR}}\right) \tag{2}\]

where \(\left\langle\right\rangle\) means the sequence concatenation. \(p_{\mathrm{NAR}}(.)\) computes the pointwise probabilities of \(\mathcal{A}_{\cdot,l}\).

### Multi-lingual Training

In order to learn cross-lingual acoustic conversion information for cross-lingual TTS and speech-to-speech translation tasks, we take advantage of bilingual speech-transcription (ASR) corpus2, pairs of (\(\mathcal{S}^{s}\), \(\mathcal{A}^{s}\)) and (\(\mathcal{S}^{t}\), \(\mathcal{A}^{t}\)) to train our multi-lingual codec LMs \(\phi_{\mathrm{MAR}}\) and \(\phi_{\mathrm{MNAR}}\), where \(s\) and \(t\) represent two different (source and target) languages.

Footnote 2: Current version of VALL-E X is trained on the speech-transcription of two languages, we leave exploring more languages for future work.

Language ID ModuleFollowing multi-lingual TTS, we leverage a language ID to guide the speech generation for specific languages in VALL-E X. On the one hand, without language ID, VALL-E X may be confused to select suitable acoustic tokens for the specific language since it is trained with multi-lingual data. On the other hand, some languages have very different characteristics, for example, Chinese is a tone language while English is a non-tone language, which increases the difficulty of adjusting the speaking style across languages. Our experiments found that adding language information to the input of our multi-lingual autoregressive codec LM \(\phi_{\mathrm{MAR}}\) is surprisingly effective in guiding the right speaking style and relieving the L2 accent problem, which will be introduced in Section 5.5. Concretely, we embed language IDs into dense vectors and add them to the embeddings of acoustic tokens.

### Cross-Lingual Inference

After training, VALL-E X can perform cross-lingual speech synthesis, as shown in Figure 3. In detail, we first concatenate source phonemes \(\mathcal{S}^{s}\) and target phonemes \(\mathcal{S}^{t}\) as prompts, and take the first-layer source acoustic tokens \(\mathcal{A}^{s}_{\cdot,1}\) as the decoding prefix, condition on which the multi-lingual autoregressive codec LM \(\phi_{\mathrm{MAR}}\) generates the first-layer target acoustic tokens \(\mathcal{A}^{t}_{\cdot,1}\),

\[\hat{a}^{t}_{\cdot,1}\sim p_{\mathrm{AR}}\left(a^{t}_{\cdot,1}\mid\left\langle \mathcal{S}^{s},\mathcal{S}^{t},\mathcal{A}^{s}_{\cdot,1},\mathcal{A}^{t}_{ \cdot<i,1}\right\rangle;\phi_{\mathrm{MAR}}\right),i=1,\ldots, \tag{3}\]

where \(\sim\) means probability-based sampling. The sampling is stopped until the <end-of-sentence> token is sampled. As mentioned in Section 3.3, language ID is used to control the speaking style of the final generated speech. After obtaining the first-layer target acoustic tokens \(\mathcal{A}^{t}_{\cdot,1}\) from \(\phi_{\mathrm{MAR}}\), multi-lingual non-autoregressive codec LM \(\phi_{\mathrm{MNAR}}\) is used to predict the rest layers of acoustic tokens \(\left\{\mathcal{A}^{t}_{\cdot,l}\mid l=2,\ldots,8\right\}\) by greedy search, i.e., choosing the tokens with maximum probabilities,

\[\mathcal{A}^{t}_{\cdot,l}=\operatorname*{argmax}_{\mathcal{A}^{t}_{\cdot,l}}p _{\mathrm{NAR}}\left(\mathcal{A}^{t}_{\cdot,l}\mid\left\langle\mathcal{S}^{t },\mathcal{A}^{s}_{\cdot,1:8},\mathcal{A}^{t}_{\cdot,1:l-1}\right\rangle; \phi_{\mathrm{MNAR}}\right),l=2,\ldots,8. \tag{4}\]

Finally, we use the decoder of EnCodec to synthesize the target speech from the complete target acoustic tokens \(\mathcal{A}^{t}_{\cdot,1:8}\).;
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = ""https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js"";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id=""content""><div id=""content-text""></div></div>
</body>
</html>
",,,2023-09-05 17:05:20.792296
E:\workspace\python\Ocr\Nougat_webui\flagged\pdf文件\2303.03926g4j5net6.pdf,11,"<iframe src=""file=static\1693909432.492151temp.html"" width=""100%"" height=""500px""></iframe>",,,2023-09-05 18:24:05.727528
